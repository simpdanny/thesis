\section{簡介}
在使用深層類神經網絡模型時，通常會讓訓練與測試時的環境設定標準一致，讓模型可以成功的學習並且應用在實務層面上。訓練時，模型從廣大、豐富甚至重複的資料中擷取關鍵資訊，結構化組織成模型的中間參數；測試時，模型通常有其核心目標，諸如分類器或是回歸器。根據不同的應用層面，測試時必須根據所學的知識，迅速或是正確的完成任務。

近年來，由於巨量數據與硬體資源的加速擴張，辛式（Hinton）提出知識蒸餾（Knowledge Distiilation, KD）的概念，把深層類神經網路的訓練期以及測試期的模型架構分開。在訓練期的時候，模型必須要有足夠強大的架構、參數，才能汲取足夠的資訊，形成厚重模型（Cumbersome Model）。針對厚重模型，設計者可以無所不用其極地使用高複雜度的優化演算法（Optimization Algorithm）、強力的控制調適子（Regularizer）、或是龐大的結合模型（Ensemble Model）來輔助訓練，讓模型可以充分學習到知識，同時保有概括化的能力。

然而厚重模型往往龐大，無法真正套用在即時（Real-time）的應用層面上。於是，利用額外一個小型、輕便、簡單的蒸餾模型（Distillation Model），將厚重模型所汲取的豐富知識，「蒸餾」給蒸餾模型。此命名取其細火慢燉、冷凝氣態精華之意，化用自化學實驗的定義：「藉由沸點的不同，選擇性地將液體混合物中的不同物質分離，萃取指定的物質。」

典型的知識蒸餾過程如下：

\begin{itemize}
\itemsep -2pt
 \item 根據豐富的訓練資料集，設定龐大的模型架構與複雜的優化演算法，訓練出厚重模型。
 \item 選定轉移資料集，可能就是訓練資料、經揀選過的精華資料或是根本沒有正確答案的非監督資料。
 \item 在轉移資料集上，將厚重模型學到的資訊，蒸餾給蒸餾模型。
\end{itemize} 

使用知識蒸餾訓練模型之前，必須先思考到底什麼樣的重要資訊，會潛藏在厚重模型裡面。考慮一個簡單的圖形數字分類器的例子，如圖\ref{chap5_digit_classifier_example}。

%TODO chap5_digit_classifier_example

假設這個分類器問題，在於分類數字2、3還有7。在訓練分類器時，給定的正確答案通常是1-hot向量，如$[1, 0, 0]$，明確地告知這張是數字為2的圖。

但是，人類在學習的道路上，更追求舉一反三的概括化能力，因此我們可能會更想要機器知道，這張標示為2的圖，到底跟3比較像，還是跟7比較像。這樣的概括能力無法在1-hot的向量裡看到，因為向量裡兩個輸出的值皆為0。這表示我們的訓練的目標裡，根本不期望模型能夠區分3和7與2這個數字的接近程度，因而與實際的學習目標：概括化能力有些差異。

根據此觀察，知識蒸餾的重點，便是希望厚重模型將其畢生所學的資訊，轉化為概括化的能力，傳授給蒸餾模型。如圖\ref{chap5_digit_classifier_example}中，厚重模型的輸出向量為$[0.98 , 10^{-6} , 10^{-11}]$，不僅能夠學習到2的正確答案，亦能夠發現3比7還來得接近正解。

在如公式\ref{eq:LCE}裡，

\section{溫度軟性最大化(Temperature Softmax)}
\section{蒸餾設定與步驟}
\section{多目標蒸餾}
\subsection{多個單語言教師模型}
\subsection{單個多語言教師模型}
\section{蒸餾與模型壓縮}
\section{實驗結果與分析}
\subsection{單語言知識蒸餾}
\subsection{多語言知識蒸餾}
\subsection{溫度}
\subsection{模型壓縮}
\subsection{綜合比較}    

