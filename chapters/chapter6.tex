\section{本論文主要的研究貢獻}

本論文旨在探討聲學模型中的深度學習架構，並且套用於多語言情境之下，探討如何增加深度學習的資料量，形成跨語言類神經網路。主要的研究貢獻如下：

\begin{itemize}
 \itemsep -2pt
 \item 本論文先回顧基本的單語言語音辨識系統架構，著重在聲學模型中的類神經網路模型，加上現在深度學習中典型的的線性整流活化函數與丟棄演算法，成功從全球音素語料庫的四個語言：西班牙語、捷克語、法語、德語，建立可供比較的單語言語音辨識系統，與全球結果比較。

 \item 提出一個多語言語音辨識系統情境，探討如何產生一種跨語言聲學模型架構，讓四種語言的資料彼此共享訓練，幫助訓練，使得整體需要的資料量變小、模型參數需求也變小。為了評估系統在多語言的表現，提出了平均詞誤率比（AWERR）來衡量整體的辨識結果。

 \item 本論文從聲學模型合併的角度切入，討論基於國際音標合併音素、基於混淆矩陣合併三連音素音態以及基於模型共享合併中間層的方式，由粗糙到細緻層層合併，成功訓練出多語言深層類神經網路，建立比單語言語音辨識系統還要好的跨語言聲學模型。

 \item 多語言語音辨識的訓練過程龐大，與應用層面有所差距，但與知識蒸餾提出的訓練－測試分離概念有所共鳴。本論文探討了單個多語言教師模型的蒸餾，和多個單語言教師模型的蒸餾，將水平共享資訊與垂直蒸餾資訊合併，合併龐大的教師模型知識與廣闊的跨語言共享知識，獲得更好的辨識結果。
\end{itemize}
\section{本論文未來研究方向}

本論文礙於研究之力，有許多可行的想法尚未實現。如以深度學習建立單語言語音辨識系統聲學模型中，已經出現不少使用卷積類神經網路（CNN）或是長短期記憶類神經網路（LSTM）的架構，呈現的數據都相當的好。在訓練深層類神經網路模型的優化演算法上，亦有不少更勝於統計式梯度降低法（SGD）的研究。

本論文使用的基準實驗仍有許多進步的空間，隨著研究蓬勃發展，基準實驗勢必得提升成更經典有效的方法，以探討多語言情境下的各種方式成效。

本論文使研究所使用的四個語言都是印歐語系的。如果要將研究內容探討到更全域的語言特色，勢必得加入更多其他語系的語言討論，甚至是稀少語言的訓練方法。

基於混淆矩陣合併三連音素音態時，以對稱相似度作為合併距離的，並且使用階層式分群法的其中一種進行合併。未來可以使用不同的合併演算法、不同的距離衡量方式，來決定如何合併三連音素音態。

除了基於隱藏式馬可夫模型的聲學模型之外，端至端（End-to-End）系統也逐漸熱門而且效果卓越，其模型中央並沒有所謂的發音單位的概念，探討如何進行多語言模型合併會是個前所未見且有趣的題目。

眼下亦沒有人提出一個對於知識蒸餾概念中的概括化資訊的量化解釋，以及概括化資訊如何幫助類神經網路進行學習。對於概括化資訊中的純理論分析，如與控制調適的關係、與資料揀選的關係，都是值得深入討論的研究部份。
